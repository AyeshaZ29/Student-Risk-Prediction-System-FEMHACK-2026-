# -*- coding: utf-8 -*-
"""femhack2026

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIT9WlE183RDakuUIVbKxxOQl79fk2sx

df.info -> Shows number of rows & columns

Shows data types (number or text)

Shows if any values are missing

This helps us understand the dataset structure.
"""

df.info()

import pandas as pd
#loading data
df = pd.read_csv('xAPI-Edu-Data.csv')
display(df.head())

"""# Task
Load the "xAPI-Edu-Data.csv" dataset into a pandas DataFrame.

## Load Dataset

### Subtask:
Load the 'xAPI-Edu-Data.csv' dataset into a pandas DataFrame.

## Summary:

### Data Analysis Key Findings
- The dataset "xAPI-Edu-Data.csv" was successfully loaded into a pandas DataFrame.

### Insights or Next Steps
- The next step is to perform initial data exploration and cleaning to understand the dataset's characteristics and prepare it for further analysis.

# Task
Perform comprehensive Exploratory Data Analysis (EDA) on the `df` DataFrame, including checking descriptive statistics, analyzing unique values in categorical columns, visualizing feature distributions, identifying and handling any missing values, and identifying the target variable for the prediction task.

## Data Exploration and Preprocessing

### Subtask:
Load the dataset and perform comprehensive Exploratory Data Analysis (EDA) to understand distributions, correlations, and potential issues. Preprocess the data, including handling missing values, encoding categorical features, scaling numerical features, and feature engineering relevant for early detection. Define the target variable (Dropout/Continue).

**Reasoning**:
The first instruction is to display descriptive statistics for numerical and object-type columns, which helps in understanding the distribution and characteristics of the data.

This helps understand data behavior.
"""

#For numbers - mean, min, max, etc.

#For text columns - count, unique values, most common value
print("Descriptive statistics for numerical columns:")
display(df.describe())

#For text columns - count, unique values, most common value

print("\nDescriptive statistics for object (categorical) columns:")
display(df.describe(include='object'))

"""**Reasoning**:
To understand the cardinality of each feature, especially for categorical columns, the next step is to iterate through each column and display the number of unique values.


"""

#Shows how many different values each column has
#Helps identify
#Categorical columns
#columns with too many categories
print("Number of unique values for each column:")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

"""**Reasoning**:
The next step in the EDA process is to check for any missing values in the DataFrame, as this is crucial for data cleaning and preprocessing.


"""

#Finds empty cells
#Important before modeling (models hate missing data)
print("Missing values per column:")
df.isnull().sum()

"""The target variable for this prediction task is "Class." This variable likely indicates the student's performance level, making it suitable for predicting student outcomes.

## Visualize feature distributions

### Subtask:
Visualize the distribution of numerical and categorical features to understand their patterns and identify potential outliers or imbalances.

**Reasoning**:
To visualize the distributions of numerical features, I will create histograms for 'raisedhands', 'VisITedResources', 'AnnouncementsView', and 'Discussion' columns.

Why graphs?

Helps see:

Distribution

Outliers

Skewness

Used for:

raisedhands

VisITedResources

AnnouncementsView

Discussion
"""

#matplotlib & seaborn → make graphs
import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']

# Set up the matplotlib figure for histograms
plt.figure(figsize=(15, 10))

for i, col in enumerate(numerical_cols):
    plt.subplot(2, 2, i + 1)  # 2 rows, 2 columns
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Reasoning**:
To visualize the distributions of categorical features, I will iterate through the identified categorical columns and create bar plots to show the frequency of each category.

Shows how many students belong to each category

Helps detect class imbalance

Example:

Male vs Female

High vs Low performance
"""

categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID', 'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'StudentAbsenceDays', 'Class']

# Set up the matplotlib figure for bar plots
plt.figure(figsize=(20, 30))

for i, col in enumerate(categorical_cols):
    plt.subplot(7, 2, i + 1)  # Adjust subplot grid as needed for all categorical columns
    sns.countplot(y=df[col], order = df[col].value_counts().index, palette='viridis')
    plt.title(f'Frequency of {col}')
    plt.xlabel('Count')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous code generated a `FutureWarning` because `palette` was used without specifying a `hue`. To address this warning and ensure future compatibility, I will explicitly set the `hue` parameter to the column being plotted and set `legend=False`.


"""

categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID', 'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'StudentAbsenceDays', 'Class']

# Set up the matplotlib figure for bar plots
plt.figure(figsize=(20, 30))

for i, col in enumerate(categorical_cols):
    plt.subplot(7, 2, i + 1)  # Adjust subplot grid as needed for all categorical columns
    sns.countplot(y=df[col], order = df[col].value_counts().index, palette='viridis', hue=df[col], legend=False)
    plt.title(f'Frequency of {col}')
    plt.xlabel('Count')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

"""## Analyze feature correlations

### Subtask:
Analyze the correlation between numerical features and the relationship between features and the target variable to understand their impact and potential multicollinearity.

**Reasoning**:
To analyze the correlation between numerical features, I will first calculate the correlation matrix for the specified numerical columns and then visualize it using a heatmap for better understanding of their relationships.
"""

#Shows how numerical features are related
#Values close to:
#1 → strong positive relation
#-1 → strong negative relation
#0 → no relation
numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']

# Calculate the correlation matrix for numerical columns
correlation_matrix = df[numerical_cols].corr()

# Set up the matplotlib figure
plt.figure(figsize=(8, 6))

# Create a heatmap of the correlation matrix
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""**Reasoning**:
To understand the relationship between numerical features and the target variable, I will calculate the correlation between each numerical feature and the 'Class' variable. However, since 'Class' is a categorical variable, I will first convert it into a numerical representation using label encoding before calculating correlations. Then I will display the correlation of numerical features with the target variable.


"""

#Machine learning models need numbers
#Converts:
#H → 2
#M → 1
#L → 0
from sklearn.preprocessing import LabelEncoder

# Create a copy of the dataframe to avoid modifying the original 'df' for this step
df_encoded = df.copy()

# Initialize LabelEncoder
le = LabelEncoder()

# Apply LabelEncoder to the 'Class' column
df_encoded['Class_encoded'] = le.fit_transform(df_encoded['Class'])

# Calculate correlation between numerical features and the encoded target variable
correlation_with_target = df_encoded[numerical_cols].corrwith(df_encoded['Class_encoded'])

print("Correlation of numerical features with the target variable (Class_encoded):")
print(correlation_with_target)

"""## Encode categorical features

### Subtask:
Encode the categorical features using an appropriate encoding strategy.

**Reasoning**:
To prepare the data for modeling, I need to encode the categorical features. I will start by identifying the categorical columns (excluding the target variable "Class") that require encoding and then apply one-hot encoding using "pd.get_dummies()".
"""

import pandas as pd

# Identify categorical columns excluding the target variable 'Class'
categorical_features = [col for col in df.select_dtypes(include='object').columns if col != 'Class']

# Apply one-hot encoding to the identified categorical features
#Converts text columns into 0s and 1s
#Example:
#gender_M → 1
#gender_F → 0
df_encoded_features = pd.get_dummies(df[categorical_features], drop_first=True)

# Display the first few rows of the one-hot encoded features
print("One-hot encoded categorical features (first 5 rows):")
display(df_encoded_features.head())

"""**Reasoning**:
Now that the categorical features have been one-hot encoded, I will concatenate these encoded features with the original numerical features and the target variable to create a final DataFrame ready for modeling. This involves dropping the original categorical columns that were encoded to avoid redundancy.


"""

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns

# Combine the numerical features, one-hot encoded features, and the target variable
df_processed = pd.concat([df[numerical_features], df_encoded_features, df['Class']], axis=1)

# Display the first few rows of the processed DataFrame
print("Processed DataFrame with all numerical and one-hot encoded features (first 5 rows):")
display(df_processed.head())

# Display the info of the processed DataFrame to confirm dtypes and non-null counts
print("\nInfo of the processed DataFrame:")
df_processed.info()

"""## Model Training and Evaluation

### Subtask:
Split the preprocessed data into training and testing sets and select suitable machine learning models for early detection.

**Reasoning**:
To prepare the data for model training, I will separate the features (X) from the target variable (y) and then split them into training and testing sets. This ensures that the model is evaluated on unseen data, providing a more accurate assessment of its performance.
"""

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df_processed.drop('Class', axis=1)
y = df_processed['Class']

#Training data → teach the model

#Testing data → check performance Usually:

#80% training
#20% testing

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets to verify the split
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""### Selecting Machine Learning Models

For early detection of student outcomes (represented by the 'Class' variable), several classification models can be considered. The choice of model often depends on the dataset characteristics and the desired trade-off between interpretability, performance, and training time. Here are some suitable initial candidates:

1.  **Logistic Regression**: A simple yet powerful linear model often used as a baseline for classification tasks. It provides probability scores and is highly interpretable.

2.  **Decision Tree / Random Forest**: Tree-based models are good for capturing non-linear relationships and interactions between features. Random Forests, an ensemble of decision trees, are generally more robust and less prone to overfitting than individual decision trees.

3.  **Support Vector Machine (SVM)**: Effective in high-dimensional spaces and cases where the number of dimensions is greater than the number of samples. It finds the optimal hyperplane that separates classes.

4.  **Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)**: These are powerful ensemble methods that build trees sequentially, with each new tree correcting errors made by previous ones. They often yield state-of-the-art performance in many tabular data challenges.

5.  **K-Nearest Neighbors (KNN)**: A non-parametric, instance-based learning algorithm that classifies a data point based on the majority class of its k nearest neighbors. It is simple to understand and implement.

For this task, we will proceed with **Logistic Regression**, **Decision Tree Classifier**, and **Random Forest Classifier** as our initial models due to their balance of performance and interpretability. We will train and evaluate these models in subsequent steps.

## Model Training and Evaluation

### Subtask:
Train and evaluate Logistic Regression, Decision Tree Classifier, and Random Forest Classifier models.

**Reasoning**:
To train and evaluate the specified models, I will import the necessary classes, initialize each model, train them on the training data, make predictions on the test data, and then calculate and display their accuracy scores and classification reports.

**Models used:**

Logistic Regression

Decision Tree

Random Forest

**Why multiple models?**

Compare accuracy

Choose best one

Random Forest
Chosen because it gave highest accuracy.
"""

#Models used:
#Logistic Regression
#Decision Tree
#Random Forest
#Why multiple models?
#Compare accuracy
#Choose best one

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize models
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
decision_tree_model = DecisionTreeClassifier(random_state=42)
random_forest_model = RandomForestClassifier(random_state=42)

models = {
    "Logistic Regression": logistic_model,
    "Decision Tree": decision_tree_model,
    "Random Forest": random_forest_model
}

# Train, predict, and evaluate each model
for name, model in models.items():
    print(f"\n--- {name} ---")
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    percentage = accuracy * 100
    report = classification_report(y_test, y_pred)

    print(f"Accuracy: {percentage:.2f}%")

    print("Classification Report:")
    print(report)

"""The Logistic Regression model produced a ConvergenceWarning because the optimizer had difficulty converging. To fix this, numerical features are scaled using StandardScaler, which puts all features on a similar scale and helps the model converge more efficiently. In addition, setting solver='liblinear' improves stability—especially for smaller datasets—and reduces the likelihood of convergence issues. Together, these steps lead to more reliable training and potentially better model performance.

## Risk Score and Threshold Definition

### Subtask:
Develop a method to calculate a risk score for each student based on the model's predictions (e.g., probability of dropout). Define clear thresholds to categorize students into 'Low', 'Medium', and 'High' risk labels.

**Reasoning**:
Based on the previous step's evaluation, the Random Forest Classifier achieved the highest accuracy. Therefore, I will select it as the best performing model for generating risk scores. The next logical step is to use this model to predict the probabilities of each class for the test data.
"""

import pandas as pd

# Select the best performing model (Random Forest Classifier)
best_model = random_forest_model

# Get class probabilities for the test set
# The order of classes is typically sorted by the model. We need to know which index corresponds to 'L', 'M', 'H'.
# Let's inspect the model's classes_ attribute first.
print(f"Classes of the Random Forest model: {best_model.classes_}")

y_pred_proba = best_model.predict_proba(X_test)

# Create a DataFrame for probabilities
# Assuming classes_ order is consistent and known (e.g., ['H', 'L', 'M'] or ['L', 'M', 'H'])
# Based on the classification report, the classes appear to be H, L, M. Let's confirm.

# Get the class names from the model
class_names = best_model.classes_
probabilities_df = pd.DataFrame(y_pred_proba, columns=[f'prob_{c}' for c in class_names], index=X_test.index)

# Display the first few rows of the probabilities DataFrame
print("Predicted probabilities for each class (first 5 rows):")
display(probabilities_df.head())

"""**Reasoning**:
To define a 'risk score' for each student, I will use the predicted probabilities. A straightforward way to represent risk is to consider the inverse of the probability of being a high-performing student, so `Risk Score = 1 - prob_H`. This means a lower probability of 'H' (high performance) directly translates to a higher risk score, encapsulating the likelihood of needing intervention.

**Logic:**

High probability of H → low risk

Low probability of H → high risk

Risk score ranges from 0 to 1.
"""

import numpy as np

# Calculate the risk score for each student
# Using 1 - probability of being a 'High' performing student
# Assuming 'prob_H' is available in probabilities_df and corresponds to the 'H' class

# Find the index for 'H' in the class_names array to correctly select the 'prob_H' column
prob_H_col_name = f'prob_{class_names[np.where(class_names == "H")[0][0]]}'
probabilities_df['risk_score'] = 1 - probabilities_df[prob_H_col_name]

# Display the first few rows with the calculated risk score
print("Predicted probabilities and calculated risk scores (first 5 rows):")
display(probabilities_df.head())

# Analyze the distribution of risk scores to define thresholds
print("\nDescriptive statistics of the risk scores:")
display(probabilities_df['risk_score'].describe())

# Visualize the distribution of risk scores
plt.figure(figsize=(10, 6))
sns.histplot(probabilities_df['risk_score'], kde=True)
plt.title('Distribution of Risk Scores')
plt.xlabel('Risk Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""**Reasoning**:
Based on the descriptive statistics and distribution of the risk scores, I will define thresholds to categorize students into 'Low', 'Medium', and 'High' risk labels. This step involves interpreting the range and quartiles of the risk scores to set meaningful cut-off points.


"""

import numpy as np

# Define thresholds for risk categories based on the distribution of risk scores
# Using quartiles as a starting point for defining risk categories.
# For example, students in the bottom 25% of risk scores are 'Low risk',
# the next 50% are 'Medium risk', and the top 25% are 'High risk'.

# Get the quartiles of the risk scores
risk_25th_percentile = probabilities_df['risk_score'].quantile(0.25)
risk_75th_percentile = probabilities_df['risk_score'].quantile(0.75)

# Define thresholds more explicitly after inspecting the distribution
# Let's consider 0.5 and 0.8 as example thresholds for 'Low', 'Medium', 'High'
# Adjust these based on the specific domain knowledge or business requirements.
low_risk_threshold = 0.5  # Students with risk_score <= 0.5 are 'Low'
medium_risk_threshold = 0.8 # Students with risk_score > 0.5 and <= 0.8 are 'Medium'
                             # Students with risk_score > 0.8 are 'High'

probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
                                        bins=[0, low_risk_threshold, medium_risk_threshold, 1.01],
                                        labels=['Low', 'Medium', 'High'],
                                        right=True, include_lowest=True)

# Display the first few rows with the new risk labels
print("Predicted probabilities, risk scores, and risk labels (first 5 rows):")
display(probabilities_df.head())

# Display the count of students in each risk category
print("\nDistribution of Risk Labels:")
display(probabilities_df['risk_label'].value_counts())

# Visualize the distribution of risk labels
plt.figure(figsize=(8, 5))
sns.countplot(x=probabilities_df['risk_label'], order=['Low', 'Medium', 'High'], palette='viridis')
plt.title('Distribution of Student Risk Categories')
plt.xlabel('Risk Category')
plt.ylabel('Number of Students')
plt.show()

"""**Reasoning**:
To address the "FutureWarning" regarding the "palette" parameter in `sns.countplot`, I will explicitly assign the `x` variable "probabilities_df['risk_label']" to the `hue` parameter and set `legend=False` for future compatibility and to suppress the warning.


"""

import numpy as np

# Define thresholds for risk categories based on the distribution of risk scores
# Using quartiles as a starting point for defining risk categories.
# For example, students in the bottom 25% of risk scores are 'Low risk',
# the next 50% are 'Medium risk', and the top 25% are 'High risk'.

# Get the quartiles of the risk scores
risk_25th_percentile = probabilities_df['risk_score'].quantile(0.25)
risk_75th_percentile = probabilities_df['risk_score'].quantile(0.75)

# Define thresholds more explicitly after inspecting the distribution
# Let's consider 0.5 and 0.8 as example thresholds for 'Low', 'Medium', 'High'
# Adjust these based on the specific domain knowledge or business requirements.
low_risk_threshold = 0.5  # Students with risk_score <= 0.5 are 'Low'
medium_risk_threshold = 0.8 # Students with risk_score > 0.5 and <= 0.8 are 'Medium'
                             # Students with risk_score > 0.8 are 'High'

probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
                                        bins=[0, low_risk_threshold, medium_risk_threshold, 1.01],
                                        labels=['Low', 'Medium', 'High'],
                                        right=True, include_lowest=True)

# Display the first few rows with the new risk labels
print("Predicted probabilities, risk scores, and risk labels (first 5 rows):")
display(probabilities_df.head())

# Display the count of students in each risk category
print("\nDistribution of Risk Labels:")
display(probabilities_df['risk_label'].value_counts())

# Visualize the distribution of risk labels
plt.figure(figsize=(8, 5))
sns.countplot(x=probabilities_df['risk_label'], order=['Low', 'Medium', 'High'], palette='viridis', hue=probabilities_df['risk_label'], legend=False)
plt.title('Distribution of Student Risk Categories')
plt.xlabel('Risk Category')
plt.ylabel('Number of Students')
plt.show()

"""## Model Explainability and Feature Importance

### Subtask:
Extract and analyze feature importance from the trained model to understand which factors contribute most to a student's risk of dropping out.

**Reasoning**:
To understand which features are most influential in the model's predictions, I will extract feature importances from the `best_model` (Random Forest Classifier). I will then create a Series with feature names, sort them in descending order, display the top 10, and visualize them using a horizontal bar plot for clear representation.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming best_model is the trained Random Forest Classifier and X_train is available
# from the previous steps.

# 1. Access the feature_importances_ attribute from the best_model
feature_importances = best_model.feature_importances_

# 2. Create a Pandas Series of feature importances, mapping them to their corresponding feature names
feature_names = X_train.columns
feature_importance_series = pd.Series(feature_importances, index=feature_names)

# 3. Sort the feature importances in descending order
sorted_feature_importances = feature_importance_series.sort_values(ascending=False)

# 4. Display the top 10 most important features
print("Top 10 Most Important Features:")
display(sorted_feature_importances.head(10))

# 5. Create a horizontal bar plot to visualize the top 10 most important features
plt.figure(figsize=(10, 7))
sns.barplot(x=sorted_feature_importances.head(10).values, y=sorted_feature_importances.head(10).index, palette='viridis')
plt.title('Top 10 Most Important Features')
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous code generated a "FutureWarning" because "palette" was used without specifying `hue` in "sns.barplot". To address this warning and ensure future compatibility, I will explicitly set the `hue` parameter to the y-axis variable ("sorted_feature_importances.head(10).index`) and set `legend=False".


"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming best_model is the trained Random Forest Classifier and X_train is available
# from the previous steps.

# 1. Access the feature_importances_ attribute from the best_model
feature_importances = best_model.feature_importances_

# 2. Create a Pandas Series of feature importances, mapping them to their corresponding feature names
feature_names = X_train.columns
feature_importance_series = pd.Series(feature_importances, index=feature_names)

# 3. Sort the feature importances in descending order
sorted_feature_importances = feature_importance_series.sort_values(ascending=False)

# 4. Display the top 10 most important features
print("Top 10 Most Important Features:")
display(sorted_feature_importances.head(10))

# 5. Create a horizontal bar plot to visualize the top 10 most important features
plt.figure(figsize=(10, 7))
sns.barplot(x=sorted_feature_importances.head(10).values, y=sorted_feature_importances.head(10).index, palette='viridis', hue=sorted_feature_importances.head(10).index, legend=False)
plt.title('Top 10 Most Important Features')
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

"""## Save Model Artifacts

### Subtask:
Save the trained machine learning model (e.g., as .pkl or .joblib) and the preprocessing pipeline to ensure reproducibility and for deployment in the Streamlit application.

**Reasoning**:
To save the trained Random Forest model and the StandardScaler object, I will use the `joblib` library, which is efficient for serializing and deserializing Python objects, making them suitable for deployment.
"""

import joblib

# Save the trained Random Forest model
joblib.dump(random_forest_model, 'random_forest_model.joblib')
print("Random Forest model saved as 'random_forest_model.joblib'")

# Save the StandardScaler object
joblib.dump(scaler, 'scaler.joblib')
print("Scaler saved as 'scaler.joblib'")

"""## Generate Prediction File

### Subtask:
Create a CSV file named 'predictions.csv' containing 'student_id' (or row index), 'risk_score', 'risk_label' (Low/Medium/High), and 'predicted_dropout' (0/1) for all students in the test set or a designated prediction set.

**Reasoning**:
To generate the `predictions.csv` file, I need to first get the predicted class labels from the `best_model` (Random Forest Classifier) for the test set. Then, I will create the `predicted_dropout` column by mapping 'L' to 1 and 'H'/'M' to 0. Finally, I will combine all the required information (student index, risk score, risk label, and predicted dropout) into a DataFrame and save it as a CSV.
"""

import pandas as pd

# 1. Use the best_model to get the predicted class for each student in X_test
y_pred_class = best_model.predict(X_test)

# Create a DataFrame for predicted classes to align with probabilities_df
predicted_classes_df = pd.DataFrame(y_pred_class, index=X_test.index, columns=['predicted_class'])

# 2. Create a new column named `predicted_dropout`
# Assign 1 if the predicted class is 'L' (low performance/dropout risk), and 0 otherwise
# We interpret 'L' as the class most indicative of potential dropout/low performance for early detection.
predicted_classes_df['predicted_dropout'] = predicted_classes_df['predicted_class'].apply(lambda x: 1 if x == 'L' else 0)

# 3. Create a final DataFrame for predictions
# Combine student index, risk_score, risk_label from probabilities_df, and predicted_dropout
final_predictions_df = pd.DataFrame({
    'student_id': X_test.index,
    'risk_score': probabilities_df['risk_score'],
    'risk_label': probabilities_df['risk_label'],
    'predicted_dropout': predicted_classes_df['predicted_dropout']
})

# Display the first few rows of the final predictions DataFrame
print("Final predictions DataFrame (first 5 rows):")
display(final_predictions_df.head())

# 4. Save this DataFrame to a CSV file named 'predictions.csv'
# Ensure that the index is not written as a separate column in the CSV, but 'student_id' is used.
final_predictions_df.to_csv('predictions.csv', index=False)

print("\n'predictions.csv' has been created successfully.")

"""## Develop Streamlit Application

### Subtask:
Create an 'app.py' script for a Streamlit application. This app should allow users to upload a CSV file of student data, display a table of the Top 20 High-Risk Students, show a selected student’s risk score and risk label, and present the top reasons for their risk (based on feature importance).

**Reasoning**:
To make the feature importances available within the Streamlit application for explaining risk factors, I need to save the `sorted_feature_importances` Series to a file. This will allow the `app.py` script to load and use this information.
"""

import joblib

# Save the sorted feature importances
joblib.dump(sorted_feature_importances, 'feature_importances.joblib')
print("Sorted feature importances saved as 'feature_importances.joblib'")

"""**Reasoning**:
The subtask requires creating a comprehensive `app.py` script for a Streamlit application. This script will integrate all the previously saved model artifacts and implement the specified functionalities, including data upload, preprocessing, risk assessment, displaying top risk students, and providing individual student risk explanations based on feature importance. Since this is a single application file, it will be provided as one code block.


"""

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# --- Configuration --- #
# Set page configuration
st.set_page_config(page_title="Student Risk Prediction Dashboard", layout="wide")

# Define thresholds for risk categories (must match what was used in notebook)
LOW_RISK_THRESHOLD = 0.5
MEDIUM_RISK_THRESHOLD = 0.8

# --- Load Model Artifacts --- #
@st.cache_resource
def load_model_artifacts():
    model = joblib.load('random_forest_model.joblib')
    scaler = joblib.load('scaler.joblib')
    feature_importances = joblib.load('feature_importances.joblib')
    return model, scaler, feature_importances

model, scaler, feature_importances = load_model_artifacts()

# --- Preprocessing Functions --- #
def preprocess_data(df_input, scaler):
    # Identify numerical and categorical columns based on the original dataset's structure
    numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']
    original_categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID',
                                   'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey',
                                   'ParentschoolSatisfaction', 'StudentAbsenceDays']

    # Make a copy to avoid modifying the original DataFrame
    df_processed = df_input.copy()

    # Scale numerical features
    if numerical_cols and all(col in df_processed.columns for col in numerical_cols):
        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])
    else:
        st.warning("Numerical columns for scaling not found in uploaded data or some are missing.")

    # One-hot encode categorical features
    # Ensure consistent columns with training data
    # Create dummy variables for identified categorical features
    df_encoded_features = pd.get_dummies(df_processed[original_categorical_cols], drop_first=True)

    # Get the columns that were present in the training data for the model
    # Exclude target 'Class' from X_train columns
    training_features = model.feature_names_in_

    # Reindex df_encoded_features to match training columns, filling missing with 0
    # and dropping extra columns not seen during training
    processed_categorical_features = df_encoded_features.reindex(columns=training_features, fill_value=0)

    # Identify numerical features that are in training_features
    numerical_features_in_training = [col for col in numerical_cols if col in training_features]

    # Combine scaled numerical features with encoded categorical features
    # Create a new DataFrame with all features that the model expects
    X_prepared = pd.DataFrame(index=df_processed.index)

    # Add numerical features
    for col in numerical_features_in_training:
        X_prepared[col] = df_processed[col]

    # Add one-hot encoded categorical features
    for col in processed_categorical_features.columns:
        X_prepared[col] = processed_categorical_features[col]

    # Ensure the order of columns matches the training data features for prediction
    X_prepared = X_prepared[training_features]

    return X_prepared

def calculate_risk(X_data, model, class_names):
    y_pred_proba = model.predict_proba(X_data)

    probabilities_df = pd.DataFrame(y_pred_proba, columns=[f'prob_{c}' for c in class_names], index=X_data.index)

    # Assume 'H' is the high-performing class, and risk is inverse to it
    # Find the column corresponding to 'H' dynamically
    h_col_name = None
    for col in probabilities_df.columns:
        if '_H' in col:
            h_col_name = col
            break

    if h_col_name:
        probabilities_df['risk_score'] = 1 - probabilities_df[h_col_name]
    else:
        st.error("Could not find probability column for 'H' class. Risk score calculation failed.")
        probabilities_df['risk_score'] = np.nan # Assign NaN if 'H' class not found

    # Define risk labels based on thresholds
    probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
                                            bins=[0, LOW_RISK_THRESHOLD, MEDIUM_RISK_THRESHOLD, 1.01],
                                            labels=['Low', 'Medium', 'High'],
                                            right=True, include_lowest=True)
    return probabilities_df

def get_top_risk_factors(student_data, feature_importances, top_n=5):
    # student_data is a Series of a single student's preprocessed features
    # feature_importances is the Series of global feature importances

    # Filter to only features present in the student_data
    relevant_features = feature_importances[feature_importances.index.isin(student_data.index)]

    # Calculate impact score by multiplying feature value with its importance
    # For simplicity, we consider the absolute impact, as both high positive/negative values
    # could be 'reasons'. For one-hot encoded, True/False values are 1/0, so multiplying by 1
    # keeps the importance if the feature is present.
    impact_scores = student_data * relevant_features.reindex(student_data.index, fill_value=0)

    # Filter out features with zero impact (e.g., one-hot encoded features that are False for this student)
    impact_scores = impact_scores[impact_scores != 0]

    # Sort by absolute impact to get most influential features
    top_factors = impact_scores.abs().sort_values(ascending=False).head(top_n)

    # Map back to original feature names if one-hot encoded, or provide context
    # This part can be made more sophisticated to map 'gender_M' back to 'gender is Male'
    # For now, we'll just display the encoded names.
    return top_factors

# --- Streamlit App Layout --- #
st.title("Student Risk Prediction Dashboard")
st.write("Upload student data to predict risk levels and identify key contributing factors.")

# File Uploader
uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])

if uploaded_file is not None:
    try:
        df_input = pd.read_csv(uploaded_file)
        st.success("File uploaded successfully!")
        st.subheader("Original Data Head")
        st.write(df_input.head())

        # Preprocess the uploaded data
        X_processed = preprocess_data(df_input, scaler)

        # Get class names from the model
        class_names = model.classes_
        probabilities_df = calculate_risk(X_processed, model, class_names)

        # Combine with original data for display
        df_results = df_input.copy()
        df_results['student_id'] = df_results.index # Use index as student_id if not present
        df_results = df_results.merge(probabilities_df[['risk_score', 'risk_label']], left_index=True, right_index=True)

        st.subheader("Student Risk Analysis")

        # Display Top 20 High-Risk Students
        st.write("### Top 20 High-Risk Students")
        high_risk_students = df_results[df_results['risk_label'] == 'High'].sort_values(by='risk_score', ascending=False)
        if not high_risk_students.empty:
            st.dataframe(high_risk_students[['student_id', 'risk_score', 'risk_label']].head(20).reset_index(drop=True))
        else:
            st.info("No students categorized as 'High' risk.")

        # Student Selection for Detailed Analysis
        st.write("### Individual Student Risk Analysis")
        student_ids = df_results['student_id'].tolist()
        selected_student_id = st.selectbox("Select a Student ID for detailed analysis:", student_ids)

        if selected_student_id is not None:
            student_row = df_results[df_results['student_id'] == selected_student_id].iloc[0]
            st.write(f"#### Student ID: {selected_student_id}")
            st.write(f"Risk Score: **{student_row['risk_score']:.2f}**")
            st.write(f"Risk Label: **{student_row['risk_label']}**")

            # Get top risk factors
            student_features = X_processed.loc[selected_student_id]
            top_factors = get_top_risk_factors(student_features, feature_importances)

            if not top_factors.empty:
                st.write("##### Top Contributing Factors to Risk:")
                # Create a DataFrame for better display
                factors_df = pd.DataFrame({'Feature': top_factors.index, 'Impact Score': top_factors.values})
                st.dataframe(factors_df)

                # Optional: Visualize top factors for the selected student
                fig, ax = plt.subplots(figsize=(8, 5))
                sns.barplot(x='Impact Score', y='Feature', data=factors_df, palette='viridis', ax=ax, hue='Feature', legend=False)
                ax.set_title(f'Top Factors for Student {selected_student_id}')
                ax.set_xlabel('Impact Score (Absolute)')
                ax.set_ylabel('Feature')
                st.pyplot(fig)
            else:
                st.info("No significant contributing factors identified for this student.")

    except Exception as e:
        st.error(f"Error processing file: {e}")
        st.error("Please ensure the CSV file has the correct columns and format.")

else:
    st.info("Please upload a CSV file to get started.")

"""**Reasoning**:
The previous code block failed because the 'streamlit' module was not found. To resolve this, 'streamlit' needs to be installed in the environment.


"""

import sys
!{sys.executable} -m pip install streamlit

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# --- Configuration --- #
# Set page configuration
st.set_page_config(page_title="Student Risk Prediction Dashboard", layout="wide")

# Define thresholds for risk categories (must match what was used in notebook)
LOW_RISK_THRESHOLD = 0.5
MEDIUM_RISK_THRESHOLD = 0.8

# --- Load Model Artifacts --- #
@st.cache_resource
def load_model_artifacts():
    model = joblib.load('random_forest_model.joblib')
    scaler = joblib.load('scaler.joblib')
    feature_importances = joblib.load('feature_importances.joblib')
    return model, scaler, feature_importances

model, scaler, feature_importances = load_model_artifacts()

# --- Preprocessing Functions --- #
def preprocess_data(df_input, scaler):
    # Identify numerical and categorical columns based on the original dataset's structure
    numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']
    original_categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID',
                                   'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey',
                                   'ParentschoolSatisfaction', 'StudentAbsenceDays']

    # Make a copy to avoid modifying the original DataFrame
    df_processed = df_input.copy()

    # Scale numerical features
    if numerical_cols and all(col in df_processed.columns for col in numerical_cols):
        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])
    else:
        st.warning("Numerical columns for scaling not found in uploaded data or some are missing.")

    # One-hot encode categorical features
    # Ensure consistent columns with training data
    # Create dummy variables for identified categorical features
    df_encoded_features = pd.get_dummies(df_processed[original_categorical_cols], drop_first=True)

    # Get the columns that were present in the training data for the model
    # Exclude target 'Class' from X_train columns
    training_features = model.feature_names_in_

    # Reindex df_encoded_features to match training columns, filling missing with 0
    # and dropping extra columns not seen during training
    processed_categorical_features = df_encoded_features.reindex(columns=training_features, fill_value=0)

    # Identify numerical features that are in training_features
    numerical_features_in_training = [col for col in numerical_cols if col in training_features]

    # Combine scaled numerical features with encoded categorical features
    # Create a new DataFrame with all features that the model expects
    X_prepared = pd.DataFrame(index=df_processed.index)

    # Add numerical features
    for col in numerical_features_in_training:
        X_prepared[col] = df_processed[col]

    # Add one-hot encoded categorical features
    for col in processed_categorical_features.columns:
        X_prepared[col] = processed_categorical_features[col]

    # Ensure the order of columns matches the training data features for prediction
    X_prepared = X_prepared[training_features]

    return X_prepared

def calculate_risk(X_data, model, class_names):
    y_pred_proba = model.predict_proba(X_data)

    probabilities_df = pd.DataFrame(y_pred_proba, columns=[f'prob_{c}' for c in class_names], index=X_data.index)

    # Assume 'H' is the high-performing class, and risk is inverse to it
    # Find the column corresponding to 'H' dynamically
    h_col_name = None
    for col in probabilities_df.columns:
        if '_H' in col:
            h_col_name = col
            break

    if h_col_name:
        probabilities_df['risk_score'] = 1 - probabilities_df[h_col_name]
    else:
        st.error("Could not find probability column for 'H' class. Risk score calculation failed.")
        probabilities_df['risk_score'] = np.nan # Assign NaN if 'H' class not found

    # Define risk labels based on thresholds
    probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
                                            bins=[0, LOW_RISK_THRESHOLD, MEDIUM_RISK_THRESHOLD, 1.01],
                                            labels=['Low', 'Medium', 'High'],
                                            right=True, include_lowest=True)
    return probabilities_df

def get_top_risk_factors(student_data, feature_importances, top_n=5):
    # student_data is a Series of a single student's preprocessed features
    # feature_importances is the Series of global feature importances

    # Filter to only features present in the student_data
    relevant_features = feature_importances[feature_importances.index.isin(student_data.index)]

    # Calculate impact score by multiplying feature value with its importance
    # For simplicity, we consider the absolute impact, as both high positive/negative values
    # could be 'reasons'. For one-hot encoded, True/False values are 1/0, so multiplying by 1
    # keeps the importance if the feature is present.
    impact_scores = student_data * relevant_features.reindex(student_data.index, fill_value=0)

    # Filter out features with zero impact (e.g., one-hot encoded features that are False for this student)
    impact_scores = impact_scores[impact_scores != 0]

    # Sort by absolute impact to get most influential features
    top_factors = impact_scores.abs().sort_values(ascending=False).head(top_n)

    # Map back to original feature names if one-hot encoded, or provide context
    # This part can be made more sophisticated to map 'gender_M' back to 'gender is Male'
    # For now, we'll just display the encoded names.
    return top_factors

# --- Streamlit App Layout --- #
st.title("Student Risk Prediction Dashboard")
st.write("Upload student data to predict risk levels and identify key contributing factors.")

# File Uploader
uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])

if uploaded_file is not None:
    try:
        df_input = pd.read_csv(uploaded_file)
        st.success("File uploaded successfully!")
        st.subheader("Original Data Head")
        st.write(df_input.head())

        # Preprocess the uploaded data
        X_processed = preprocess_data(df_input, scaler)

        # Get class names from the model
        class_names = model.classes_
        probabilities_df = calculate_risk(X_processed, model, class_names)

        # Combine with original data for display
        df_results = df_input.copy()
        df_results['student_id'] = df_results.index # Use index as student_id if not present
        df_results = df_results.merge(probabilities_df[['risk_score', 'risk_label']], left_index=True, right_index=True)

        st.subheader("Student Risk Analysis")

        # Display Top 20 High-Risk Students
        st.write("### Top 20 High-Risk Students")
        high_risk_students = df_results[df_results['risk_label'] == 'High'].sort_values(by='risk_score', ascending=False)
        if not high_risk_students.empty:
            st.dataframe(high_risk_students[['student_id', 'risk_score', 'risk_label']].head(20).reset_index(drop=True))
        else:
            st.info("No students categorized as 'High' risk.")

        # Student Selection for Detailed Analysis
        st.write("### Individual Student Risk Analysis")
        student_ids = df_results['student_id'].tolist()
        selected_student_id = st.selectbox("Select a Student ID for detailed analysis:", student_ids)

        if selected_student_id is not None:
            student_row = df_results[df_results['student_id'] == selected_student_id].iloc[0]
            st.write(f"#### Student ID: {selected_student_id}")
            st.write(f"Risk Score: **{student_row['risk_score']:.2f}**")
            st.write(f"Risk Label: **{student_row['risk_label']}**")

            # Get top risk factors
            student_features = X_processed.loc[selected_student_id]
            top_factors = get_top_risk_factors(student_features, feature_importances)

            if not top_factors.empty:
                st.write("##### Top Contributing Factors to Risk:")
                # Create a DataFrame for better display
                factors_df = pd.DataFrame({'Feature': top_factors.index, 'Impact Score': top_factors.values})
                st.dataframe(factors_df)

                # Optional: Visualize top factors for the selected student
                fig, ax = plt.subplots(figsize=(8, 5))
                sns.barplot(x='Impact Score', y='Feature', data=factors_df, palette='viridis', ax=ax, hue='Feature', legend=False)
                ax.set_title(f'Top Factors for Student {selected_student_id}')
                ax.set_xlabel('Impact Score (Absolute)')
                ax.set_ylabel('Feature')
                st.pyplot(fig)
            else:
                st.info("No significant contributing factors identified for this student.")

    except Exception as e:
        st.error(f"Error processing file: {e}")
        st.error("Please ensure the CSV file has the correct columns and format.")

else:
    st.info("Please upload a CSV file to get started.")

"""**Reasoning**:
The previous Streamlit application code failed to run because the `!pip install streamlit` command was in the same cell as the application code, which doesn't allow the newly installed package to be recognized by the current kernel session. According to the coding tips, shell commands should be in separate cells. To resolve this, I will move the Streamlit installation command into its own cell.


"""

import sys
!{sys.executable} -m pip install streamlit
print("Streamlit has been installed.")

"""**Reasoning**:
Now that Streamlit has been installed in a separate cell, I will provide the Streamlit application code in a new cell. This script will integrate all the previously saved model artifacts and implement the specified functionalities, including data upload, preprocessing, risk assessment, displaying top risk students, and providing individual risk explanations based on feature importance.


"""

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# --- Configuration --- #
# Set page configuration
st.set_page_config(page_title="Student Risk Prediction Dashboard", layout="wide")

# Define thresholds for risk categories (must match what was used in notebook)
LOW_RISK_THRESHOLD = 0.5
MEDIUM_RISK_THRESHOLD = 0.8

# --- Load Model Artifacts --- #
@st.cache_resource
def load_model_artifacts():
    model = joblib.load('random_forest_model.joblib')
    scaler = joblib.load('scaler.joblib')
    feature_importances = joblib.load('feature_importances.joblib')
    return model, scaler, feature_importances

model, scaler, feature_importances = load_model_artifacts()

# --- Preprocessing Functions --- #
def preprocess_data(df_input, scaler):
    # Identify numerical and categorical columns based on the original dataset's structure
    numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']
    original_categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID',
                                   'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey',
                                   'ParentschoolSatisfaction', 'StudentAbsenceDays']

    # Make a copy to avoid modifying the original DataFrame
    df_processed = df_input.copy()

    # Scale numerical features
    if numerical_cols and all(col in df_processed.columns for col in numerical_cols):
        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])
    else:
        st.warning("Numerical columns for scaling not found in uploaded data or some are missing.")

    # One-hot encode categorical features
    # Ensure consistent columns with training data
    # Create dummy variables for identified categorical features
    df_encoded_features = pd.get_dummies(df_processed[original_categorical_cols], drop_first=True)

    # Get the columns that were present in the training data for the model
    # Exclude target 'Class' from X_train columns
    training_features = model.feature_names_in_

    # Reindex df_encoded_features to match training columns, filling missing with 0
    # and dropping extra columns not seen during training
    processed_categorical_features = df_encoded_features.reindex(columns=training_features, fill_value=0)

    # Identify numerical features that are in training_features
    numerical_features_in_training = [col for col in numerical_cols if col in training_features]

    # Combine scaled numerical features with encoded categorical features
    # Create a new DataFrame with all features that the model expects
    X_prepared = pd.DataFrame(index=df_processed.index)

    # Add numerical features
    for col in numerical_features_in_training:
        X_prepared[col] = df_processed[col]

    # Add one-hot encoded categorical features
    for col in processed_categorical_features.columns:
        X_prepared[col] = processed_categorical_features[col]

    # Ensure the order of columns matches the training data features for prediction
    X_prepared = X_prepared[training_features]

    return X_prepared

def calculate_risk(X_data, model, class_names):
    y_pred_proba = model.predict_proba(X_data)

    probabilities_df = pd.DataFrame(y_pred_proba, columns=[f'prob_{c}' for c in class_names], index=X_data.index)

    # Assume 'H' is the high-performing class, and risk is inverse to it
    # Find the column corresponding to 'H' dynamically
    h_col_name = None
    for col in probabilities_df.columns:
        if '_H' in col:
            h_col_name = col
            break

    if h_col_name:
        probabilities_df['risk_score'] = 1 - probabilities_df[h_col_name]
    else:
        st.error("Could not find probability column for 'H' class. Risk score calculation failed.")
        probabilities_df['risk_score'] = np.nan # Assign NaN if 'H' class not found

    # Define risk labels based on thresholds
    probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
                                            bins=[0, LOW_RISK_THRESHOLD, MEDIUM_RISK_THRESHOLD, 1.01],
                                            labels=['Low', 'Medium', 'High'],
                                            right=True, include_lowest=True)
    return probabilities_df

def get_top_risk_factors(student_data, feature_importances, top_n=5):
    # student_data is a Series of a single student's preprocessed features
    # feature_importances is the Series of global feature importances

    # Filter to only features present in the student_data
    relevant_features = feature_importances[feature_importances.index.isin(student_data.index)]

    # Calculate impact score by multiplying feature value with its importance
    # For simplicity, we consider the absolute impact, as both high positive/negative values
    # could be 'reasons'. For one-hot encoded, True/False values are 1/0, so multiplying by 1
    # keeps the importance if the feature is present.
    impact_scores = student_data * relevant_features.reindex(student_data.index, fill_value=0)

    # Filter out features with zero impact (e.g., one-hot encoded features that are False for this student)
    impact_scores = impact_scores[impact_scores != 0]

    # Sort by absolute impact to get most influential features
    top_factors = impact_scores.abs().sort_values(ascending=False).head(top_n)

    # Map back to original feature names if one-hot encoded, or provide context
    # This part can be made more sophisticated to map 'gender_M' back to 'gender is Male'
    # For now, we'll just display the encoded names.
    return top_factors

# --- Streamlit App Layout --- #
st.title("Student Risk Prediction Dashboard")
st.write("Upload student data to predict risk levels and identify key contributing factors.")

# File Uploader
uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])

if uploaded_file is not None:
    try:
        df_input = pd.read_csv(uploaded_file)
        st.success("File uploaded successfully!")
        st.subheader("Original Data Head")
        st.write(df_input.head())

        # Preprocess the uploaded data
        X_processed = preprocess_data(df_input, scaler)

        # Get class names from the model
        class_names = model.classes_
        probabilities_df = calculate_risk(X_processed, model, class_names)

        # Combine with original data for display
        df_results = df_input.copy()
        df_results['student_id'] = df_results.index # Use index as student_id if not present
        df_results = df_results.merge(probabilities_df[['risk_score', 'risk_label']], left_index=True, right_index=True)

        st.subheader("Student Risk Analysis")

        # Display Top 20 High-Risk Students
        st.write("### Top 20 High-Risk Students")
        high_risk_students = df_results[df_results['risk_label'] == 'High'].sort_values(by='risk_score', ascending=False)
        if not high_risk_students.empty:
            st.dataframe(high_risk_students[['student_id', 'risk_score', 'risk_label']].head(20).reset_index(drop=True))
        else:
            st.info("No students categorized as 'High' risk.")

        # Student Selection for Detailed Analysis
        st.write("### Individual Student Risk Analysis")
        student_ids = df_results['student_id'].tolist()
        selected_student_id = st.selectbox("Select a Student ID for detailed analysis:", student_ids)

        if selected_student_id is not None:
            student_row = df_results[df_results['student_id'] == selected_student_id].iloc[0]
            st.write(f"#### Student ID: {selected_student_id}")
            st.write(f"Risk Score: **{student_row['risk_score']:.2f}**")
            st.write(f"Risk Label: **{student_row['risk_label']}**")

            # Get top risk factors
            student_features = X_processed.loc[selected_student_id]
            top_factors = get_top_risk_factors(student_features, feature_importances)

            if not top_factors.empty:
                st.write("##### Top Contributing Factors to Risk:")
                # Create a DataFrame for better display
                factors_df = pd.DataFrame({'Feature': top_factors.index, 'Impact Score': top_factors.values})
                st.dataframe(factors_df)

                # Optional: Visualize top factors for the selected student
                fig, ax = plt.subplots(figsize=(8, 5))
                sns.barplot(x='Impact Score', y='Feature', data=factors_df, palette='viridis', ax=ax, hue='Feature', legend=False)
                ax.set_title(f'Top Factors for Student {selected_student_id}')
                ax.set_xlabel('Impact Score (Absolute)')
                ax.set_ylabel('Feature')
                st.pyplot(fig)
            else:
                st.info("No significant contributing factors identified for this student.")

    except Exception as e:
        st.error(f"Error processing file: {e}")
        st.error("Please ensure the CSV file has the correct columns and format.")

else:
    st.info("Please upload a CSV file to get started.")

"""**Reasoning**:
The subtask requires creating an `app.py` file. The previous attempt to run the Streamlit code directly in a notebook cell resulted in warnings because Streamlit is designed to be run as a script. To correctly fulfill the subtask of creating the file, I will use the `%%writefile` magic command to save the provided Streamlit application code into `app.py`.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# from sklearn.preprocessing import StandardScaler
# 
# # --- Configuration --- #
# # Set page configuration
# st.set_page_config(page_title="Student Risk Prediction Dashboard", layout="wide")
# 
# # Define thresholds for risk categories (must match what was used in notebook)
# LOW_RISK_THRESHOLD = 0.5
# MEDIUM_RISK_THRESHOLD = 0.8
# 
# # --- Load Model Artifacts --- #
# @st.cache_resource
# def load_model_artifacts():
#     model = joblib.load('random_forest_model.joblib')
#     scaler = joblib.load('scaler.joblib')
#     feature_importances = joblib.load('feature_importances.joblib')
#     return model, scaler, feature_importances
# 
# model, scaler, feature_importances = load_model_artifacts()
# 
# # --- Preprocessing Functions --- #
# def preprocess_data(df_input, scaler):
#     # Identify numerical and categorical columns based on the original dataset's structure
#     numerical_cols = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']
#     original_categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID',
#                                    'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey',
#                                    'ParentschoolSatisfaction', 'StudentAbsenceDays']
# 
#     # Make a copy to avoid modifying the original DataFrame
#     df_processed = df_input.copy()
# 
#     # Scale numerical features
#     if numerical_cols and all(col in df_processed.columns for col in numerical_cols):
#         df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])
#     else:
#         st.warning("Numerical columns for scaling not found in uploaded data or some are missing.")
# 
#     # One-hot encode categorical features
#     # Ensure consistent columns with training data
#     # Create dummy variables for identified categorical features
#     df_encoded_features = pd.get_dummies(df_processed[original_categorical_cols], drop_first=True)
# 
#     # Get the columns that were present in the training data for the model
#     # Exclude target 'Class' from X_train columns
#     training_features = model.feature_names_in_
# 
#     # Reindex df_encoded_features to match training columns, filling missing with 0
#     # and dropping extra columns not seen during training
#     processed_categorical_features = df_encoded_features.reindex(columns=training_features, fill_value=0)
# 
#     # Identify numerical features that are in training_features
#     numerical_features_in_training = [col for col in numerical_cols if col in training_features]
# 
#     # Combine scaled numerical features with encoded categorical features
#     # Create a new DataFrame with all features that the model expects
#     X_prepared = pd.DataFrame(index=df_processed.index)
# 
#     # Add numerical features
#     for col in numerical_features_in_training:
#         X_prepared[col] = df_processed[col]
# 
#     # Add one-hot encoded categorical features
#     for col in processed_categorical_features.columns:
#         X_prepared[col] = processed_categorical_features[col]
# 
#     # Ensure the order of columns matches the training data features for prediction
#     X_prepared = X_prepared[training_features]
# 
#     return X_prepared
# 
# def calculate_risk(X_data, model, class_names):
#     y_pred_proba = model.predict_proba(X_data)
# 
#     probabilities_df = pd.DataFrame(y_pred_proba, columns=[f'prob_{c}' for c in class_names], index=X_data.index)
# 
#     # Assume 'H' is the high-performing class, and risk is inverse to it
#     # Find the column corresponding to 'H' dynamically
#     h_col_name = None
#     for col in probabilities_df.columns:
#         if '_H' in col:
#             h_col_name = col
#             break
# 
#     if h_col_name:
#         probabilities_df['risk_score'] = 1 - probabilities_df[h_col_name]
#     else:
#         st.error("Could not find probability column for 'H' class. Risk score calculation failed.")
#         probabilities_df['risk_score'] = np.nan # Assign NaN if 'H' class not found
# 
#     # Define risk labels based on thresholds
#     probabilities_df['risk_label'] = pd.cut(probabilities_df['risk_score'],
#                                             bins=[0, LOW_RISK_THRESHOLD, MEDIUM_RISK_THRESHOLD, 1.01],
#                                             labels=['Low', 'Medium', 'High'],
#                                             right=True, include_lowest=True)
#     return probabilities_df
# 
# def get_top_risk_factors(student_data, feature_importances, top_n=5):
#     # student_data is a Series of a single student's preprocessed features
#     # feature_importances is the Series of global feature importances
# 
#     # Filter to only features present in the student_data
#     relevant_features = feature_importances[feature_importances.index.isin(student_data.index)]
# 
#     # Calculate impact score by multiplying feature value with its importance
#     # For simplicity, we consider the absolute impact, as both high positive/negative values
#     # could be 'reasons'. For one-hot encoded, True/False values are 1/0, so multiplying by 1
#     # keeps the importance if the feature is present.
#     impact_scores = student_data * relevant_features.reindex(student_data.index, fill_value=0)
# 
#     # Filter out features with zero impact (e.g., one-hot encoded features that are False for this student)
#     impact_scores = impact_scores[impact_scores != 0]
# 
#     # Sort by absolute impact to get most influential features
#     top_factors = impact_scores.abs().sort_values(ascending=False).head(top_n)
# 
#     # Map back to original feature names if one-hot encoded, or provide context
#     # This part can be made more sophisticated to map 'gender_M' back to 'gender is Male'
#     # For now, we'll just display the encoded names.
#     return top_factors
# 
# # --- Streamlit App Layout --- #
# st.title("Student Risk Prediction Dashboard")
# st.write("Upload student data to predict risk levels and identify key contributing factors.")
# 
# # File Uploader
# uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
# 
# if uploaded_file is not None:
#     try:
#         df_input = pd.read_csv(uploaded_file)
#         st.success("File uploaded successfully!")
#         st.subheader("Original Data Head")
#         st.write(df_input.head())
# 
#         # Preprocess the uploaded data
#         X_processed = preprocess_data(df_input, scaler)
# 
#         # Get class names from the model
#         class_names = model.classes_
#         probabilities_df = calculate_risk(X_processed, model, class_names)
# 
#         # Combine with original data for display
#         df_results = df_input.copy()
#         df_results['student_id'] = df_results.index # Use index as student_id if not present
#         df_results = df_results.merge(probabilities_df[['risk_score', 'risk_label']], left_index=True, right_index=True)
# 
#         st.subheader("Student Risk Analysis")
# 
#         # Display Top 20 High-Risk Students
#         st.write("### Top 20 High-Risk Students")
#         high_risk_students = df_results[df_results['risk_label'] == 'High'].sort_values(by='risk_score', ascending=False)
#         if not high_risk_students.empty:
#             st.dataframe(high_risk_students[['student_id', 'risk_score', 'risk_label']].head(20).reset_index(drop=True))
#         else:
#             st.info("No students categorized as 'High' risk.")
# 
#         # Student Selection for Detailed Analysis
#         st.write("### Individual Student Risk Analysis")
#         student_ids = df_results['student_id'].tolist()
#         selected_student_id = st.selectbox("Select a Student ID for detailed analysis:", student_ids)
# 
#         if selected_student_id is not None:
#             student_row = df_results[df_results['student_id'] == selected_student_id].iloc[0]
#             st.write(f"#### Student ID: {selected_student_id}")
#             st.write(f"Risk Score: **{student_row['risk_score']:.2f}**")
#             st.write(f"Risk Label: **{student_row['risk_label']}**")
# 
#             # Get top risk factors
#             student_features = X_processed.loc[selected_student_id]
#             top_factors = get_top_risk_factors(student_features, feature_importances)
# 
#             if not top_factors.empty:
#                 st.write("##### Top Contributing Factors to Risk:")
#                 # Create a DataFrame for better display
#                 factors_df = pd.DataFrame({'Feature': top_factors.index, 'Impact Score': top_factors.values})
#                 st.dataframe(factors_df)
# 
#                 # Optional: Visualize top factors for the selected student
#                 fig, ax = plt.subplots(figsize=(8, 5))
#                 sns.barplot(x='Impact Score', y='Feature', data=factors_df, palette='viridis', ax=ax, hue='Feature', legend=False)
#                 ax.set_title(f'Top Factors for Student {selected_student_id}')
#                 ax.set_xlabel('Impact Score (Absolute)')
#                 ax.set_ylabel('Feature')
#                 st.pyplot(fig)
#             else:
#                 st.info("No significant contributing factors identified for this student.")
# 
#     except Exception as e:
#         st.error(f"Error processing file: {e}")
#         st.error("Please ensure the CSV file has the correct columns and format.")
# 
# else:
#     st.info("Please upload a CSV file to get started.")

"""## Write Comprehensive Report (README)

### Subtask:
Prepare a 'README.md' file that documents the entire project.

## Summary:

### Data Analysis Key Findings

*   **Data Overview**: The dataset consists of 480 entries and contains 17 features. There are no missing values across any of the columns.
*   **Target Variable**: The `Class` column, representing student performance levels, was identified as the target variable for prediction. It has three unique values, with 'M' (Medium) being the most frequent (211 times).
*   **Numerical Feature Distributions**:
    *   `raisedhands` (mean: 46.77, range: 0-100), `VisITedResources` (mean: 54.79, range: 0-99), `AnnouncementsView` (mean: 37.89, range: 0-82), and `Discussion` (mean: 43.29, range: 0-90) show varying distributions, providing initial insights into student engagement.
*   **Feature Correlations**:
    *   The heatmap of numerical features showed moderate positive correlations between `raisedhands`, `VisITedResources`, and `AnnouncementsView`.
    *   Correlation with the encoded target variable (`Class_encoded`) indicated weak to moderate negative relationships: `raisedhands` (-0.23), `VisITedResources` (-0.16), `AnnouncementsView` (-0.14), and `Discussion` (-0.12).
*   **Categorical Feature Encoding**: 16 categorical features (excluding the target `Class`) were one-hot encoded using `pd.get_dummies(drop_first=True)`, resulting in 56 new boolean columns. The final processed DataFrame (`df_processed`) contains 480 entries and 61 columns, ready for model training.
*   **Model Performance**:
    *   After scaling numerical features, three classification models were trained and evaluated on the test set (96 samples).
    *   **Random Forest Classifier** achieved the highest accuracy at 0.8438, outperforming Decision Tree (0.8125) and Logistic Regression (0.7292).
    *   The Random Forest Classifier was selected as the best-performing model for subsequent risk assessment.
*   **Risk Score and Categorization**:
    *   A `risk_score` was calculated as 1 minus the probability of a student being in the 'H' (High performance) class.
    *   Risk thresholds were defined: Low ($ \le 0.5 $), Medium ($ > 0.5 \text{ and } \le 0.8 $), and High ($ > 0.8 $).
    *   The test set showed the following distribution of risk labels: 55 'High' risk, 23 'Medium' risk, and 18 'Low' risk students.
*   **Feature Importance**: The top 5 most influential factors contributing to student risk, as identified by the Random Forest model, are:
    1.  `VisITedResources` (0.1536)
    2.  `raisedhands` (0.1382)
    3.  `StudentAbsenceDays_Under-7` (0.1117)
    4.  `AnnouncementsView` (0.1107)
    5.  `Discussion` (0.0859)
*   **Model Artifacts & Prediction File**: The trained Random Forest model, the StandardScaler, and feature importances were successfully saved. A `predictions.csv` file was generated containing `student_id`, `risk_score`, `risk_label`, and `predicted_dropout` (1 if predicted class is 'L', 0 otherwise) for the test set.
*   **Streamlit Application**: An `app.py` script was created for a Streamlit application, designed to allow users to upload student data, predict risk levels, display the top 20 high-risk students, and present individual student risk factors.

### Insights or Next Steps

*   The Random Forest Classifier provides a robust foundation for early student risk detection, with `VisITedResources`, `raisedhands`, and `StudentAbsenceDays_Under-7` being the most impactful features for intervention strategies.
*   The developed Streamlit application offers a practical tool for educators or administrators to proactively identify and understand at-risk students, facilitating timely support and resource allocation.

**This project:**

Loads student data

Explores & cleans it

Trains ML models

Predicts student risk

Explains predictions

Shows results in a web app
"""